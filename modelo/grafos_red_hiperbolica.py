# -*- coding: utf-8 -*-
"""Grafos_Red_Hiperbolica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CJfy4X3oxTUFxLuEmyUjOGixXVBD5b4k
"""

!pip install geoopt

import torch
import torch.nn.functional as F

def build_FSG(features, k=8):
    """
    Construye el Feature Similarity Graph (FSG) basado en similitud coseno.
    features: tensor [T, D]
    Retorna: matriz de adyacencia binaria [T, T]
    """
    with torch.no_grad():
        normed = F.normalize(features, p=2, dim=1)  # [T, D]
        sim_matrix = torch.matmul(normed, normed.T)  # coseno [T, T]

        # Elimina la diagonal (no conectar consigo mismo)
        sim_matrix.fill_diagonal_(0)

        # Top-k por fila
        topk = torch.topk(sim_matrix, k=k, dim=1).indices
        adj = torch.zeros_like(sim_matrix)

        for i in range(features.size(0)):
            adj[i, topk[i]] = 1
        adj = torch.maximum(adj, adj.T)  # aseguramos simetría

    return adj  # [T, T]

def build_TRG(T, d=2):
    """
    Construye el Temporal Relation Graph (TRG) con ventanas de vecindad.
    T: número de segmentos
    d: ventana (e.g., 2 conecta a vecinos i±1, i±2)
    Retorna: matriz de adyacencia binaria [T, T]
    """
    adj = torch.zeros((T, T))
    for i in range(T):
        for j in range(max(0, i - d), min(T, i + d + 1)):
            if i != j:
                adj[i, j] = 1
    return adj

!pip install torch==2.1.0 torchvision==0.16.0 --quiet
!pip install scipy==1.10.1 --quiet
!pip install git+https://github.com/geoopt/geoopt.git@v0.5.0 --quiet

import geoopt
print("Geoopt OK:", geoopt.__version__)

import torch
import geoopt
from torch import nn

def create_ball(ball=None, c=None):
    if ball is None:
        assert c is not None, "curvature of the ball should be explicitly specified"
        ball = geoopt.PoincareBall(c)
    return ball

def mobius_linear(input, weight, bias=None, nonlin=None, *, ball: geoopt.PoincareBall):
    output = ball.mobius_matvec(weight, input)
    if bias is not None:
        output = ball.mobius_add(output, bias)
    if nonlin is not None:
        output = ball.logmap0(output)
        output = nonlin(output)
        output = ball.expmap0(output)
    return output

class MobiusLinear(nn.Linear):
    def __init__(self, *args, nonlin=None, ball=None, c=1.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.ball = create_ball(ball, c)
        if self.bias is not None:
            self.bias = geoopt.ManifoldParameter(self.bias, manifold=self.ball)
        self.nonlin = nonlin
        self.reset_parameters()

    def forward(self, input):
        return mobius_linear(
            input,
            weight=self.weight,
            bias=self.bias,
            nonlin=self.nonlin,
            ball=self.ball,
        )

    @torch.no_grad()
    def reset_parameters(self):
        torch.nn.init.eye_(self.weight)
        self.weight.add_(torch.rand_like(self.weight).mul_(1e-3))
        if self.bias is not None:
            self.bias.zero_()

class HyperbolicGraphConvolution(nn.Module):
    def __init__(self, in_dim, out_dim, c=1.0):
        super().__init__()
        self.ball = geoopt.PoincareBall(c)
        self.linear = MobiusLinear(in_dim, out_dim, ball=self.ball, nonlin=torch.nn.Tanh())
        self.manifold = self.ball

    def forward(self, x, adj):
        x_h = self.manifold.expmap0(x)
        x_h = self.manifold.projx(x_h)

        deg = adj.sum(1).clamp(min=1).unsqueeze(1)
        agg = torch.matmul(adj, x_h) / deg
        agg = self.manifold.projx(agg)

        out = self.linear(agg)
        return self.manifold.projx(out)

class HyperbolicVideoAnomalyModel(nn.Module):
    def __init__(self, T=32, D=158, hidden_dim=64, K=5, c=1.0):
        super().__init__()
        self.T = T
        self.D = D
        self.K = K
        self.ball = geoopt.PoincareBall(c)

        self.hgcn_fsg = HyperbolicGraphConvolution(D, hidden_dim, c=c)
        self.hgcn_trg = HyperbolicGraphConvolution(D, hidden_dim, c=c)

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * K, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x, adj_fsg, adj_trg):
        B = x.size(0)
        outputs = []

        for i in range(B):
            x_i = x[i]
            fsg_i = adj_fsg[i]
            trg_i = adj_trg[i]

            h1 = self.hgcn_fsg(x_i, fsg_i)
            h2 = self.hgcn_trg(x_i, trg_i)
            h = (h1 + h2) / 2
            h = self.ball.logmap0(h)

            norms = torch.norm(h, dim=1)
            topk = torch.topk(norms, self.K).indices
            top_feat = h[topk]
            vec = top_feat.reshape(-1)

            out = self.classifier(vec)
            outputs.append(out)

        return torch.stack(outputs).squeeze(1)

import os
import json
import torch
import numpy as np
from glob import glob
from torch.utils.data import Dataset, DataLoader


class VideoFeatureDataset(Dataset):
    def __init__(self, npy_dir, labels_json, k_fsg=8, d_trg=2, fixed_len=32):
        with open(labels_json, 'r') as f:
            self.labels = json.load(f)

        self.files = []
        for file in sorted(glob(os.path.join(npy_dir, "*.npy"))):
            name = os.path.splitext(os.path.basename(file))[0]
            if name in self.labels:
                self.files.append(file)

        self.k = k_fsg
        self.d = d_trg
        self.fixed_len = fixed_len

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file = self.files[idx]
        name = os.path.splitext(os.path.basename(file))[0]
        x = np.load(file).astype(np.float32)  # [T_var, D]
        y = self.labels[name]

        # Ajustar tamaño T
        if x.shape[0] > self.fixed_len:
            x = x[:self.fixed_len]
        elif x.shape[0] < self.fixed_len:
            pad = np.zeros((self.fixed_len - x.shape[0], x.shape[1]), dtype=np.float32)
            x = np.vstack([x, pad])

        x = torch.tensor(x)
        y = torch.tensor(y, dtype=torch.float32)
        adj_fsg = build_FSG(x, k=self.k)
        adj_trg = build_TRG(self.fixed_len, d=self.d)

        return x, adj_fsg, adj_trg, y

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for x, fsg, trg, y in dataloader:
            preds = model(x, fsg, trg)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    # Obtener mejor threshold basado en curva ROC
    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)
    optimal_idx = (tpr - fpr).argmax()
    optimal_threshold = thresholds[optimal_idx]

    # Usar ese threshold para binarizar las predicciones
    preds_binary = [1 if p >= optimal_threshold else 0 for p in all_preds]

    # Calcular métricas
    acc = accuracy_score(all_labels, preds_binary)
    prec = precision_score(all_labels, preds_binary, zero_division=0)
    rec = recall_score(all_labels, preds_binary, zero_division=0)
    f1 = f1_score(all_labels, preds_binary, zero_division=0)
    auc = roc_auc_score(all_labels, all_preds)

    return acc, prec, rec, f1, auc

import os
import csv
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix


def train_with_validation(model, train_loader, val_loader, save_path="best_model.pt",
                          log_path="train_log.csv", epochs=20, lr=1e-4, patience=50):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.BCELoss()
    best_val_loss = float("inf")
    patience_counter = 0

    # Inicializar CSV si no existe
    if not os.path.exists(log_path):
        with open(log_path, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["epoch", "train_loss", "val_loss", "accuracy", "precision", "recall", "f1", "auc"])

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for x, fsg, trg, y in train_loader:
            preds = model(x, fsg, trg)
            loss = criterion(preds, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        val_loss = 0
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for x, fsg, trg, y in val_loader:
                preds = model(x, fsg, trg)
                loss = criterion(preds, y)
                val_loss += loss.item()
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(y.cpu().numpy())

        # Métricas
        preds_bin = [1 if p >= 0.5 else 0 for p in all_preds]
        acc = accuracy_score(all_labels, preds_bin)
        prec = precision_score(all_labels, preds_bin, zero_division=0)
        rec = recall_score(all_labels, preds_bin, zero_division=0)
        f1 = f1_score(all_labels, preds_bin, zero_division=0)
        auc = roc_auc_score(all_labels, all_preds)

        print(f"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | "
              f"Acc: {acc:.2f} | Prec: {prec:.2f} | Rec: {rec:.2f} | F1: {f1:.2f} | AUC: {auc:.2f}")
        # Visualizar e imprimir la matriz de confusión
        plot_confusion_matrix(all_labels, preds_bin, epoch+1, save_path=f"/content/drive/MyDrive/aaa/conf_epoch_{epoch+1}.png")
        print("Matriz de confusión:")
        print(confusion_matrix(all_labels, preds_bin))

        # Guardar métricas al CSV
        with open(log_path, "a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([epoch+1, train_loss, val_loss, acc, prec, rec, f1, auc])

        # Guardar mejor modelo
        if val_loss < best_val_loss:
            patience_counter = 0
            best_val_loss = val_loss
            torch.save(model.state_dict(), save_path)
            print("Modelo mejor guardado")
        else:
          patience_counter += 1
          if patience_counter > patience:
             print(f"Entrenamiento cerrado por early stopping, no mejoras en {patience} epocas.")
             break

    print("Entrenamiento finalizado.")

def plot_confusion_matrix(y_true, y_pred, epoch, save_path=None):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=["Normal", "Anomaly"], yticklabels=["Normal", "Anomaly"])
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"Confusion Matrix - Epoch {epoch}")
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150)
    plt.show()

from google.colab import drive
drive.mount('/content/drive')

# Rutas
train_dir = "/content/drive/MyDrive/aaa/entrenamiento"
train_json = "/content/drive/MyDrive/aaa/train_labels.json"

val_dir = "/content/drive/MyDrive/aaa/validacion"
val_json = "/content/drive/MyDrive/aaa/val_labels.json"

test_dir = "/content/drive/MyDrive/aaa/prueba"
test_json = "/content/drive/MyDrive/aaa/test_labels.json"

test_ds = VideoFeatureDataset(test_dir, test_json)
test_loader = DataLoader(test_ds, batch_size=4)

# Cargar datasets
train_ds = VideoFeatureDataset(train_dir, train_json)
val_ds = VideoFeatureDataset(val_dir, val_json)

train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=4)

# Crear modelo
model = HyperbolicVideoAnomalyModel()

# Entrenar
train_with_validation(model, train_loader, val_loader,log_path="/content/drive/MyDrive/aaa/train_log.csv", save_path="/content/drive/MyDrive/aaa/best_model.pt", epochs=100)

print(f"Entrenamiento: {len(train_ds)} muestras")
print(f"Validación: {len(val_ds)} muestras")
print(f"Prueba: {len(test_ds)} muestras")