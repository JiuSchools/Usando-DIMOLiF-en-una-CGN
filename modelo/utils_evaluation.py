# -*- coding: utf-8 -*-
"""utils/evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mdGyjqAoa7rlRgt-5LpZjOdyE8pjj0hU
"""

import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

def evaluate_model(model, dataloader):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for x, fsg, trg, y in dataloader:
            preds = model(x, fsg, trg)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)
    optimal_threshold = thresholds[(tpr - fpr).argmax()]
    preds_binary = [1 if p >= optimal_threshold else 0 for p in all_preds]
    acc = accuracy_score(all_labels, preds_binary)
    prec = precision_score(all_labels, preds_binary, zero_division=0)
    rec = recall_score(all_labels, preds_binary, zero_division=0)
    f1 = f1_score(all_labels, preds_binary, zero_division=0)
    auc = roc_auc_score(all_labels, all_preds)
    return acc, prec, rec, f1, auc